NLP (Natural Language Processing)自然语言处理
1、语义角度
2、字面角度

垂类：垂直分类，比如旅游网站、视频网站、音乐网站……专一某一领域的

余弦相似度
	根据向量点积公式 
		a、b两个向量 a(x1,x2) b(y1,y2)
		cosθ=a·b/||a||||b||
		a·b = x1x2+y1y2 数量积（又叫内积、点积）
		||a||||b|| = √(x1²+y2²) * √(x1²+y2²)
	余弦值的范围在[-1,1]之间，值越趋近于1，代表两个向量的方向越接近；越趋近于-1，他们的方向越相反；接近于0，表示两个向量近乎于正交。
	

停用词(stop words) 如：的、是…… 
常见词、较常见词、较少见词
TF （Term Frequency）词频
	TF = 某个词在文章中出现的次数/文章总词数
	TF = 某个词在文章中出现的次数/该文章出现次数最多的词出现次数
		 
IDF (inverse document frequency)反文档频率,逆向文件频率
	IDF = log(语料库的文档总数/(包含该词的文档数+1))
	x = a^y(a>0,a!=1)  y=logax a叫做对数的底数，x叫做真数，y叫做“以a为底x的对数”。
	x越小y越小
TF-IDF = TF * IDF 越大代表对文章越重要
思想：
	TF计算关键词在一篇文档的频率
	IDF计算在许多文档中，文档使用关键词的频率，频率越小说明关键词平常人们使用的不多，所以越关键
	合在一起就可以计算关键词对于使用关键词的文档的重要性

该算法在数据挖掘、文本处理和信息检索等领域得到了广泛的应用，如从一篇文章中找到它的关键词。
TFIDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。

TF-IDF是一种统计方法，用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜寻引擎应用，作为文件与用户查询之间相关程度的度量或评级。除了TF-IDF以外，因特网上的搜寻引擎还会使用基于连结分析的评级方法，以确定文件在搜寻结果中出现的顺序。
	
	
	 
	
	